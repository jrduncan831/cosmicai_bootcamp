{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this tutorial we will walk through how to fine-tune a (as of 2024) medium-sized large language model, specifically, the 2 billion parameter version of Google's Gemma 2 model. OpenAI's GPT4 and Meta's largest Llama3 models are in the trillion and hundred billion parameter range respectively, but such models are likely overkill for many use cases and are too large to train on the GPUs available to us on Frontera for this course. From my qualitative assesment, the 8 billion parameter version of Llama3 appears to be the most popular choice for finetuning, likely because it's one of the newest models to be released and during training it can just barely fit in the GPU memory available on a single consumer GPU such as NVIDIA's RTX 4090. \n",
    "\n",
    "# 1. Torchtune library initialization\n",
    "The torchtune python library we will be using to fine-tune our LLM in this tutorial has a collection of fine tuning \"recipes\" for many of the most popular LLM models available including the Llama family and Mistral models. It should be straight forward to adapt what we cover here to these other models should your specific use case require that. The torchtune documentation can be viewed at https://pytorch.org/torchtune/stable/index.html.\n",
    "\n",
    "First let's make sure torchtune is installed and can initialize. Running the following command will give a brief overview of the functionality of the torchtune library: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tune [-h] {download,ls,cp,run,validate} ...\r\n",
      "\r\n",
      "Welcome to the torchtune CLI!\r\n",
      "\r\n",
      "options:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "\r\n",
      "subcommands:\r\n",
      "  {download,ls,cp,run,validate}\r\n",
      "    download            Download a model from the Hugging Face Hub.\r\n",
      "    ls                  List all built-in recipes and configs\r\n",
      "    cp                  Copy a built-in recipe or config to a local path.\r\n",
      "    run                 Run a recipe. For distributed recipes, this supports\r\n",
      "                        all torchrun arguments.\r\n",
      "    validate            Validate a config and ensure that it is well-formed.\r\n"
     ]
    }
   ],
   "source": [
    "! tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Downloading model files\n",
    "You can download models directly using torchtune with a single command however it requires you to setup an account with the model repository huggingface.co, so, for convinience, we have already downloaded the model weights and tokenizer for the 2 billion parameter version of Google's Gemma2 model to the directory `/work2/10156/gj3385/frontera/ml_institute/models/gemma-2b/` for use in this course. The parent directory also contains a fine-tuned version of gemma-2b that we will use later on.\n",
    "\n",
    "We will now copy the model files to the temp directory on our current node for fast file access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p /tmp/models\n",
    "! cp -r /scratch1/10156/gj3385/ml_institute/models/. /tmp/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that our model files copied over, running the following command should show two subdirectories:\n",
    "+ gemma-2b (our base model)\n",
    "+ gemma-2b-lora-finetuned-alpaca (base fine-tuned on alpaca dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma-2b  gemma-2b-lora-finetuned-alpaca\r\n"
     ]
    }
   ],
   "source": [
    "! ls /tmp/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**For when you want to download other models on your own time**\n",
    "1. Make an account on https://huggingface.co/join and login\n",
    "2. Find the model you want, e.g. https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
    "3. Some models are \"gated\" and require you to submit information about yourself and accept a licensing agreement before being granted access.  For the example above, there is a form you can fill out on the model card page.\n",
    "4. You'll recieve a confirmation email when your access has been granted\n",
    "5. On huggingface, click on your profile icon and go to “*Settings*” and then “*Access Tokens*” and click “*Create New Token*”. Be sure to click the checkbox for “*read access to contents of all public gated repos you can access*” and then scroll to the bottom and \"*Create Token*\". You will use the provided token string in your torchtune download command.\n",
    "6. If you lose your token string, you’ll have to go back to your \"*Access Tokens*\" page which lists your current tokens, find the token you just made, and on the far right click the three little dots, then select “*invalidate and refresh*” to get the new token string (there’s no way to view the old token string)\n",
    "\n",
    "**Example download command in torchtune:** \n",
    "\n",
    "`! tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir [directory you want model to go] --hf-token [your hugging face token]`\n",
    "    \n",
    "----\n",
    "\n",
    "# 3. Model fine-tuning recipe selection\n",
    "Running the command below will retrieve a list of the preconfigured training, generation, and evaluation recipes available to us in torchtune. In the RECIPE column, a \"full_finetune\" means training all the model weights (this is memory and computationally expensive), we will instead being doing a \"lora_finetune\" which will only train the weights of a small lora added to the model.  The \"single_device\" recipes run on a single GPU, \"distributed\" recipes run the training across multiple GPUs. There are differences between each model's architecture so the CONFIG column shows default configuration files for specific models for each recipe. In this course we will be running:\n",
    "\n",
    "RECIPE: **lora_finetune_distributed**  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; CONFIG: **gemma/2B_lora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECIPE                                   CONFIG                                  \r\n",
      "full_finetune_single_device              llama2/7B_full_low_memory               \r\n",
      "                                         code_llama2/7B_full_low_memory          \r\n",
      "                                         llama3/8B_full_single_device            \r\n",
      "                                         llama3_1/8B_full_single_device          \r\n",
      "                                         mistral/7B_full_low_memory              \r\n",
      "                                         phi3/mini_full_low_memory               \r\n",
      "                                         qwen2/7B_full_single_device             \r\n",
      "                                         qwen2/0.5B_full_single_device           \r\n",
      "                                         qwen2/1.5B_full_single_device           \r\n",
      "full_finetune_distributed                llama2/7B_full                          \r\n",
      "                                         llama2/13B_full                         \r\n",
      "                                         llama3/8B_full                          \r\n",
      "                                         llama3_1/8B_full                        \r\n",
      "                                         llama3/70B_full                         \r\n",
      "                                         llama3_1/70B_full                       \r\n",
      "                                         mistral/7B_full                         \r\n",
      "                                         gemma/2B_full                           \r\n",
      "                                         gemma/7B_full                           \r\n",
      "                                         phi3/mini_full                          \r\n",
      "                                         qwen2/7B_full                           \r\n",
      "                                         qwen2/0.5B_full                         \r\n",
      "                                         qwen2/1.5B_full                         \r\n",
      "lora_finetune_single_device              llama2/7B_lora_single_device            \r\n",
      "                                         llama2/7B_qlora_single_device           \r\n",
      "                                         code_llama2/7B_lora_single_device       \r\n",
      "                                         code_llama2/7B_qlora_single_device      \r\n",
      "                                         llama3/8B_lora_single_device            \r\n",
      "                                         llama3_1/8B_lora_single_device          \r\n",
      "                                         llama3/8B_qlora_single_device           \r\n",
      "                                         llama3/8B_dora_single_device            \r\n",
      "                                         llama3/8B_qdora_single_device           \r\n",
      "                                         llama3_1/8B_qlora_single_device         \r\n",
      "                                         llama2/13B_qlora_single_device          \r\n",
      "                                         mistral/7B_lora_single_device           \r\n",
      "                                         mistral/7B_qlora_single_device          \r\n",
      "                                         gemma/2B_lora_single_device             \r\n",
      "                                         gemma/2B_qlora_single_device            \r\n",
      "                                         gemma/7B_lora_single_device             \r\n",
      "                                         gemma/7B_qlora_single_device            \r\n",
      "                                         phi3/mini_lora_single_device            \r\n",
      "                                         phi3/mini_qlora_single_device           \r\n",
      "                                         qwen2/7B_lora_single_device             \r\n",
      "                                         qwen2/0.5B_lora_single_device           \r\n",
      "                                         qwen2/1.5B_lora_single_device           \r\n",
      "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \r\n",
      "lora_dpo_distributed                     llama2/7B_lora_dpo                      \r\n",
      "ppo_full_finetune_single_device          mistral/7B_full_ppo_low_memory          \r\n",
      "lora_finetune_distributed                llama2/7B_lora                          \r\n",
      "                                         llama2/13B_lora                         \r\n",
      "                                         llama2/70B_lora                         \r\n",
      "                                         llama2/7B_qlora                         \r\n",
      "                                         llama2/70B_qlora                        \r\n",
      "                                         llama3/8B_dora                          \r\n",
      "                                         llama3/70B_lora                         \r\n",
      "                                         llama3_1/70B_lora                       \r\n",
      "                                         llama3/8B_lora                          \r\n",
      "                                         llama3_1/8B_lora                        \r\n",
      "                                         mistral/7B_lora                         \r\n",
      "                                         gemma/2B_lora                           \r\n",
      "                                         gemma/7B_lora                           \r\n",
      "                                         phi3/mini_lora                          \r\n",
      "                                         qwen2/7B_lora                           \r\n",
      "                                         qwen2/0.5B_lora                         \r\n",
      "                                         qwen2/1.5B_lora                         \r\n",
      "generate                                 generation                              \r\n",
      "eleuther_eval                            eleuther_evaluation                     \r\n",
      "quantize                                 quantization                            \r\n",
      "qat_distributed                          llama2/7B_qat_full                      \r\n",
      "                                         llama3/8B_qat_full                      \r\n"
     ]
    }
   ],
   "source": [
    "! tune ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Configuring the Training Dataset\n",
    "In the following section we will look at a common example dataset called *alpaca* that comes preintegrated into torchtune as well as a method to build your own custom datasets.\n",
    "\n",
    "<h2><center>4.1 Initialize the Tokenizer</center></h2>\n",
    "First we need to initialize the tokenizer for the specific model we plan to finetune. The tokenizer breaks up the words in an input text string into small groups of characters and assigns each of these groupings a unique number which is refferred to as a <em>token</em> or <em>token id</em>. Note that it's not quite a 1 word = 1 token, some words will be broken up into several tokens and often special characters get their own token. The range of unique tokens ids the tokenizer produces is often refered to as the <em>vocabulary</em> of the model. For this example, we will be using torchtune's implementation of the gemma model family tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:   [2, 214614, 2616, 708, 10740, 235341, 1]\n",
      "Characters:  ['', 'Lla', 'mas', ' are', ' awesome', '!', '']\n"
     ]
    }
   ],
   "source": [
    "from torchtune.models.gemma import gemma_tokenizer\n",
    "\n",
    "# Initialize the gemma tokenizer from a saved file\n",
    "tokenizer = gemma_tokenizer(\n",
    "    path=\"/tmp/models/gemma-2b/tokenizer.model\",\n",
    "    max_seq_len=512)\n",
    "\n",
    "# Let's feed some text to the tokenizer to see that it's working\n",
    "text = \"Llamas are awesome!\"\n",
    "tokenized_text = tokenizer.encode(text)\n",
    "print(\"Token IDs:  \", tokenized_text)\n",
    "\n",
    "# Let's decode the tokens back to normal text one by one to see what they each represent\n",
    "chars_per_token = []\n",
    "for token in tokenized_text:\n",
    "    chars_per_token.append(tokenizer.decode(token))\n",
    "    \n",
    "print(\"Characters: \", chars_per_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2><center>4.2 Alpaca Packing - The Alpaca Dataset</center></h2>\n",
    "Now lets load in the alpaca dataset so we can look at how sample packing works and what the samples in the dataset look like. For illustrative purposes, we'll load in the dataset twice; once without sample packing, and a second time with sample packing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Packing dataset: 100%|██████████| 52002/52002 [01:19<00:00, 655.69it/s] \n"
     ]
    }
   ],
   "source": [
    "from torchtune.datasets import alpaca_dataset, PackedDataset\n",
    "\n",
    "# Instantiate the alpaca dataset but don't pack the samples\n",
    "dataset_no_packing = alpaca_dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    packed=False,\n",
    ")\n",
    "\n",
    "# Instantiate the alpaca dataset again and pack the samples\n",
    "dataset_packed = alpaca_dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    packed=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of multiple samples.  Each sample contains an instruction (prompt) and a response (what we hope the model will generate). During training, we will feed to the model one sample at a time with the response obscured and see how well the model can guess the expected response. Each sample in this dataset follows the same instruct format:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    \n",
    "<br>\\### Instruction:\n",
    "{instruction}\n",
    "\n",
    "\\### Response:\n",
    "{response}\n",
    "</div>\n",
    "\n",
    "\n",
    "Let's print the first two samples in the dataset. Note that the dataset has already been encoded using the tokenizer, so we have to decode each sample to get the human readable text back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n",
      "\n",
      "\n",
      "Sample 2:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the three primary colors?\n",
      "\n",
      "### Response:\n",
      "The three primary colors are red, blue, and yellow.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 2 samples from the unpacked dataset\n",
    "for i, sample in enumerate(dataset_no_packing):\n",
    "    \n",
    "    # break after i=2\n",
    "    if i >= 2:\n",
    "        break\n",
    "        \n",
    "    # print current sample #\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    \n",
    "    # pull out tokens from current sample\n",
    "    tokens = sample['tokens']\n",
    "    \n",
    "    # decode tokens and print resulting text\n",
    "    print(f\"{tokenizer.decode(tokens)}\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was anyone else bothered by the lack of a space character after the \"1.\" in the first sample? I was. Now let's print just the first sample from the packed dataset, notice how many prompts and responses have been packed into this single sample! There are also new tensors embedded in the packed dataset object for the positional encoding and attention masking to ensure that when this packed sample is sent to the model, the model's generated output properly addresses each individual prompt in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the three primary colors?\n",
      "\n",
      "### Response:\n",
      "The three primary colors are red, blue, and yellow.Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the structure of an atom.\n",
      "\n",
      "### Response:\n",
      "An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "How can we reduce air pollution?\n",
      "\n",
      "### Response:\n",
      "There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print the first sample from the packed dataset\n",
    "for i, sample in enumerate(dataset_packed):\n",
    "    \n",
    "    # break after i=1\n",
    "    if i >= 1:\n",
    "        break\n",
    "    \n",
    "    # print current sample number\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    \n",
    "    # pull out tokens for this sample, convert tensor to list before decoding\n",
    "    tokens = sample['tokens'].tolist()\n",
    "\n",
    "    # decode tokens and print resulting text\n",
    "    print(f\"{tokenizer.decode(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example dataset file in json formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "\t{\r\n",
      "\t\t\"prompt\": \"What time is it in London?\",\r\n",
      "\t\t\"response\": \"It is 10:00 AM in London.\",\r\n",
      "\t},\r\n",
      "\t{\r\n",
      "\t\t\"prompt\": \"How many bananas?\",\r\n",
      "\t\t\"response\": \"All the bananas\",\r\n",
      "\t}\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "! cat ./config_files/custom_json_dataset.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will show you how to load the data from our custom dataset file into a dataset object in torchtune which can then be used to train any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "System:You are an AI assistant.\\n\\nUser:What time is it in London?\\n\\nAssistant:It is 10:00 AM in London.\\n\\n\n",
      "Sample 2:\n",
      "System:You are an AI assistant.\\n\\nUser:How many bananas?\\n\\nAssistant:All the bananas\\n\\n\n"
     ]
    }
   ],
   "source": [
    "from torchtune.models.gemma import gemma_tokenizer\n",
    "from torchtune.datasets import instruct_dataset\n",
    "\n",
    "# Define our prompt template which follows the format\n",
    "# \n",
    "# {<role>: (<prepended text>,<appended text>),...}\n",
    "# \n",
    "# The roles our dataset will recognize by default are system, user, and assistant\n",
    "custom_template = {\n",
    "    \"system\": (\"System: \", \"\\\\n\\\\n\"),\n",
    "    \"user\": (\"User: \", \"\\\\n\\\\n\"),\n",
    "    \"assistant\": (\"Assistant: \", \"\\\\n\\\\n\"),\n",
    "}\n",
    "\n",
    "# Initialize the gemma tokenizer with our template\n",
    "tokenizer = gemma_tokenizer(\n",
    "    path=\"/tmp/models/gemma-2b/tokenizer.model\",\n",
    "    prompt_template=custom_template)\n",
    "\n",
    "# Load our dataset as an instruct_dataset\n",
    "# Note that the new_system_prompt variable will define our system message for all samples.\n",
    "# The column_map will map key values from our json data to the input and output labels.\n",
    "# The tokenizer will then map input to the user role, and output to the assistant role\n",
    "# when using our prompt template (yes it's confusing)\n",
    "custom_dataset = instruct_dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    source=\"json\",\n",
    "    data_files=\"./config_files/custom_json_dataset.json\",\n",
    "    split=\"train\",\n",
    "    \n",
    "    # By default, user prompt is ignored in loss. Set to True to include it\n",
    "    train_on_input=True,\n",
    "    \n",
    "    # System message to use for every sample\n",
    "    new_system_prompt=\"You are an AI assistant. \",\n",
    "    \n",
    "    # Map the key 'prompt' from our dataset to 'input', and the key 'response' to 'output'\n",
    "    column_map={\"input\": \"prompt\", \"output\": \"response\"},\n",
    ")\n",
    "\n",
    "# Let's decode the samples in the dataset and print them to see that the formatting worked\n",
    "for i, sample in enumerate(custom_dataset):\n",
    "    \n",
    "    # print current sample #\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    \n",
    "    # pull out the token values for this sample, and the masking labels\n",
    "    tokens, labels = sample[\"tokens\"], sample[\"labels\"]\n",
    "    print(tokenizer.decode(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Training\n",
    "We will be using the command line interface (CLI) of torchtune to run our model training. The basic syntax is\n",
    "\n",
    "`! tune run <recipe args> <recipe name> <configuration file> <optional config overrides>`\n",
    "\n",
    "As an example, \n",
    "\n",
    "`! tune run --nnodes 1 --nproc_per_node 4 lora_finetune_distributed --config gemma/2B_lora epochs=1`\n",
    "\n",
    "This would run a lora fine-tune on 1 node distributed across 4 GPUs using the default gemma/2B configuration with a configuration override to only train for 1 epoch. We'll provide pre-customized configuration files below, but if you want to make your own, it's easiest to copy the default configuration file and modify it to your needs. To do so, You can run the command\n",
    "\n",
    "`! tune cp gemma/2B_lora /path/to/save/clever_config_name.yaml`\n",
    "\n",
    "Note that you can change *gemma/2B_lora* to whatever recipe you want to copy.  We have already prepared configuration files for you, let's take a look at the contents of the lora training configuration file below by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Config for multi-device LoRA finetuning in lora_finetune_distributed.py\r\n",
      "# using a gemma 2B model\r\n",
      "#\r\n",
      "# To launch on 4 devices, run the following command from root:\r\n",
      "#   tune run --nnodes 1 --nproc_per_node 4 lora_finetune_distributed --config ./config_files/custom_train_gemma2-2b-lora.yaml\r\n",
      "#\r\n",
      "# You can add specific overrides through the command line. For example\r\n",
      "# to override the checkpointer directory while launching training\r\n",
      "# you can run:\r\n",
      "#   tune run --nnodes 1 --nproc_per_node 4 lora_finetune_distributed --config ./config_files/custom_train_gemma2-2b-lora.yaml  checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>\r\n",
      "#\r\n",
      "# This config works only when the model is being fine-tuned on 2+ GPUs.\r\n",
      "\r\n",
      "\r\n",
      "# Tokenizer\r\n",
      "tokenizer:\r\n",
      "  _component_: torchtune.models.gemma.gemma_tokenizer\r\n",
      "  path: /tmp/models/gemma-2b/tokenizer.model\r\n",
      "  max_seq_len: 512\r\n",
      "\r\n",
      "# Dataset\r\n",
      "dataset:\r\n",
      "  _component_: torchtune.datasets.alpaca_dataset\r\n",
      "  packed: true\r\n",
      "seed: null\r\n",
      "shuffle: True\r\n",
      "\r\n",
      "\r\n",
      "# Model Arguments\r\n",
      "model:\r\n",
      "  _component_: torchtune.models.gemma.lora_gemma_2b\r\n",
      "  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']\r\n",
      "  apply_lora_to_mlp: True\r\n",
      "  lora_rank: 64\r\n",
      "  lora_alpha: 16\r\n",
      "  lora_dropout: 0.0\r\n",
      "\r\n",
      "checkpointer:\r\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\r\n",
      "  checkpoint_dir: /tmp/models/gemma-2b/\r\n",
      "  checkpoint_files: [\r\n",
      "    model-00001-of-00002.safetensors,\r\n",
      "    model-00002-of-00002.safetensors,\r\n",
      "  ]\r\n",
      "  recipe_checkpoint: null\r\n",
      "  output_dir: ./models/gemma-2b_lora_tutorial\r\n",
      "  model_type: GEMMA\r\n",
      "resume_from_checkpoint: False\r\n",
      "save_adapter_weights_only: False\r\n",
      "\r\n",
      "optimizer:\r\n",
      "  _component_: torch.optim.AdamW\r\n",
      "  lr: 2e-5\r\n",
      "\r\n",
      "lr_scheduler:\r\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\r\n",
      "  num_warmup_steps: 100\r\n",
      "\r\n",
      "loss:\r\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\r\n",
      "\r\n",
      "# Fine-tuning arguments\r\n",
      "batch_size: 2\r\n",
      "epochs: 1\r\n",
      "max_steps_per_epoch: null\r\n",
      "gradient_accumulation_steps: 1\r\n",
      "\r\n",
      "# Training env\r\n",
      "device: cuda\r\n",
      "\r\n",
      "# Memory management\r\n",
      "enable_activation_checkpointing: True\r\n",
      "\r\n",
      "# Reduced precision\r\n",
      "dtype: bf16\r\n",
      "\r\n",
      "# Logging\r\n",
      "metric_logger:\r\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\r\n",
      "  log_dir: ${output_dir}\r\n",
      "output_dir: /tmp/alpaca-gemma-lora\r\n",
      "log_every_n_steps: 1\r\n",
      "log_peak_memory_stats: False\r\n"
     ]
    }
   ],
   "source": [
    "! cat ./config_files/custom_train_gemma2-2b-lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>5.1 Training Visualization Setup - Weights and Biases</center></h2>\n",
    "By default, torchtune configuration files are set to log training progress to a local file, but you can also have it upload training statistics to a popular online visualizer called <a href=\"https://wandb.ai/site\">Weights&Biases</a>. You will need to make an account on their website to use the visualizer, they have a free tier which is sufficient for what we will be doing here. You can also apply for an free academic license. Once you have your account, you can get a copy of your api key here: <a href=\"https://wandb.ai/authorize\">https://wandb.ai/authorize</a>.\n",
    "\n",
    "The following code below will authorize the connection between this notebook session and your wandb account.  Paste your api into the box when prompted and hit ENTER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgjaffe\u001b[0m (\u001b[33mgjaffe-university-of-texas-at-ausitn\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>5.2 Run LoRA fine-tune</center></h2>\n",
    "The command below will run the LoRA fine-tune using our custom configuration file.  Note that it takes ~1.5 hours to complete 1 epoch on the full dataset, so for illustrative purposes we have added in the override\n",
    "\n",
    "`dataset.split=\"train[:1%]\"`\n",
    "\n",
    "which will run the training on just 1% of the dataset. The recipe will first initialize the model into memory, pack the dataset, fine-tune the model, and then save the new model weights to a file.  For convinience, we've already run and saved the fine-tuned weights so you can skip ahead to the generation section if you don't want to wait for this to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with torchrun...\n",
      "Running LoRAFinetuneRecipeDistributed with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tmp/models/gemma-2b/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00002.safetensors\n",
      "  - model-00002-of-00002.safetensors\n",
      "  model_type: GEMMA\n",
      "  output_dir: /work2/10156/gj3385/frontera/gemma2-lora/\n",
      "  recipe_checkpoint: null\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_dataset\n",
      "  packed: true\n",
      "  split: train[:1%]\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: false\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 100\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  log_dir: /tmp/alpaca-gemma-lora\n",
      "  project: torchtune\n",
      "model:\n",
      "  _component_: torchtune.models.gemma.lora_gemma_2b\n",
      "  apply_lora_to_mlp: true\n",
      "  lora_alpha: 16\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  lora_dropout: 0.0\n",
      "  lora_rank: 64\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  lr: 2.0e-05\n",
      "output_dir: /tmp/alpaca-gemma-lora\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.gemma.gemma_tokenizer\n",
      "  max_seq_len: 512\n",
      "  path: /tmp/models/gemma-2b/tokenizer.model\n",
      "\n",
      "Setting manual seed to local seed 1362357691. Local seed is seed + rank = 1362357691 + 0\n",
      "Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgjaffe\u001b[0m (\u001b[33mgjaffe-university-of-texas-at-ausitn\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /tmp/alpaca-gemma-lora/wandb/ wasn't writable, using system temp directory.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/tmp/wandb/run-20250303_144346-ykwb8l5h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgiddy-dew-17\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/gjaffe-university-of-texas-at-ausitn/torchtune\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/gjaffe-university-of-texas-at-ausitn/torchtune/runs/ykwb8l5h\u001b[0m\n",
      "Logging /tmp/models/gemma-2b/torchtune_config.yaml to W&B under Files\n",
      "FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "Instantiating model and loading checkpoint took 6.11 secs\n",
      "Memory stats after model init:\n",
      "\tGPU peak memory allocation: 1.40 GiB\n",
      "\tGPU peak memory reserved: 1.44 GiB\n",
      "\tGPU peak memory active: 1.40 GiB\n",
      "Optimizer is initialized.\n",
      "Loss is initialized.\n",
      "Packing dataset: 100%|███████████████████████| 520/520 [00:03<00:00, 155.36it/s]\n",
      "Dataset and Sampler are initialized.\n",
      "Learning rate scheduler is initialized.\n",
      " Profiling disabled.\n",
      " Profiler config after instantiation: {'enabled': False}\n",
      "  0%|                                                    | 0/15 [00:00<?, ?it/s]Using flex attention for attention computation since a BlockMask was passed in.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Quadro RTX 5000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Quadro RTX 5000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Quadro RTX 5000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Quadro RTX 5000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "1|15|Loss: 2.1904561519622803: 100%|████████████| 15/15 [01:02<00:00,  3.76s/it]Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "Getting full model state dict took 2.70 secs\n",
      "Model checkpoint of size 4.61 GiB saved to /work2/10156/gj3385/frontera/gemma2-lora/epoch_0/ft-model-00001-of-00002.safetensors\n",
      "Model checkpoint of size 0.06 GiB saved to /work2/10156/gj3385/frontera/gemma2-lora/epoch_0/ft-model-00002-of-00002.safetensors\n",
      "Adapter checkpoint of size 0.14 GiB saved to /work2/10156/gj3385/frontera/gemma2-lora/epoch_0/adapter_model.pt\n",
      "Adapter checkpoint of size 0.14 GiB saved to /work2/10156/gj3385/frontera/gemma2-lora/epoch_0/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to /work2/10156/gj3385/frontera/gemma2-lora/epoch_0/adapter_config.json\n",
      "Skipping copying /tmp/models/gemma-2b/gemma-2b.gguf to /work2/10156/gj3385/frontera/gemma2-lora/epoch_0 as it exceeds the size limit of 100 MiB.\n",
      "Saving final epoch checkpoint.\n",
      "The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "Saving checkpoint took 209.09 secs\n",
      "1|15|Loss: 2.1904561519622803: 100%|████████████| 15/15 [04:34<00:00, 18.30s/it]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      loss ▄▃█▄▇▆▂█▅▂▇▆▂▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        lr ▁▁▂▂▃▄▄▄▅▅▆▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: tokens_per_second_per_gpu ▁▆▇▇▇▇▇██▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      loss 2.19046\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: tokens_per_second_per_gpu 235.08536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgiddy-dew-17\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/gjaffe-university-of-texas-at-ausitn/torchtune/runs/ykwb8l5h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/gjaffe-university-of-texas-at-ausitn/torchtune\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m/tmp/wandb/run-20250303_144346-ykwb8l5h/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! tune run --nnodes 1 --nproc_per_node 4 lora_finetune_distributed \\\n",
    "--config ./config_files/custom_train_gemma2-2b-lora.yaml \\\n",
    "metric_logger._component_=torchtune.training.metric_logging.WandBLogger \\\n",
    "metric_logger.project=torchtune \\\n",
    "dataset.split=\"train[:1%]\" \\\n",
    "checkpointer.output_dir=$WORK/gemma2-lora/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model is saved to a folder on your work directory which we can view with the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adapter_0.pt\t     adapter_model.bin\thf_model_0001_0.pt\r\n",
      "adapter_config.json  config.json\thf_model_0002_0.pt\r\n"
     ]
    }
   ],
   "source": [
    "! ls $WORK/gemma2-lora/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adapter_0.pt file contains just the weights for the LoRA. The two files hf_model_0001_0.pt and hf_model_0002_0.pt contain the full model weights with the LoRA weights baked in. You'll notice there is no tokenizer model in this folder, so if we want to use this trained model we'll need to reference the tokenizer from the original base model folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Tutorial-March-2025",
   "language": "python",
   "name": "jupyter_kernel_config"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
