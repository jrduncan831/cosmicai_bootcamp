{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0db963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import argparse\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11832075",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "# parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')\n",
    "# parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "#                     help='discount factor (default: 0.99)')\n",
    "# parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "#                     help='random seed (default: 543)')\n",
    "# parser.add_argument('--render', action='store_true',\n",
    "#                     help='render the environment')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                     help='interval between training status logs (default: 10)')\n",
    "# args = parser.parse_args()\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "# env.reset(seed=seed)\n",
    "# torch.manual_seed(seed)\n",
    "stat, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477530eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = deque()\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.appendleft(R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744829bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b4b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "    \n",
    "first_episode_images = []\n",
    "last_episode_images = []\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        global last_episode_images\n",
    "        last_episode_images = []\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "#             if args.render:\n",
    "#                 env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if i_episode == 0:\n",
    "                first_episode_images.append(env.render())\n",
    "            elif i_episode != 0:\n",
    "                last_episode_images.append(env.render())\n",
    "\n",
    "            if done:\n",
    "                episode_durations.append(t+1)\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        plot_durations()\n",
    "        \n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "#             print(\"Solved! Running reward is now {} and \"\n",
    "#                   \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print('Complete')\n",
    "    plot_durations(show_result=True)\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a74922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_first_and_last_episode_gifs(first_episode_images, last_episode_images):\n",
    "    import imageio\n",
    "    imageio.mimwrite(\"./images/reinforce_cartpole_first_episode.gif\", first_episode_images, format=\"gif\",loop=0,duration=50)\n",
    "    imageio.mimwrite(\"./images/reinforce_cartpole_last_episode.gif\", last_episode_images, format=\"gif\", loop=0, duration=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a82b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_episode(episode_filename):\n",
    "    from IPython.display import display, Image\n",
    "    with open(episode_filename, 'rb') as f:\n",
    "        display(Image(f.read(), format='gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of first episode (in frames): \", len(first_episode_images))\n",
    "print(\"Length of last episode (in frames): \", len(last_episode_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_first_and_last_episode_gifs(first_episode_images, last_episode_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ba20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_episode('./images/reinforce_cartpole_first_episode.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88747163",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_episode('./images/reinforce_cartpole_last_episode.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92882282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_course_container",
   "language": "python",
   "name": "cnn_course_container"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
