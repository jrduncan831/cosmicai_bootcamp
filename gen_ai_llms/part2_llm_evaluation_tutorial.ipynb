{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this tutorial we will walk through how to send prompts and recieve generated text back from our base gemma-2b and lora fine-tuned model.  We will also demonstrate how to run evaluation benchmarks for comparing the performance of our model to others.\n",
    "\n",
    "# 1. Prompting the model\n",
    "The \"generate\" recipe in torchtune will let you send text prompts to a model.  In the sections below we will send the same prompt to the base model and the fine-tuned one to get an idea of what our fine-tuning accomplished.\n",
    "\n",
    "<h2><center>1.1 Prompting the base model</center></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running InferenceRecipe with resolved config:\n",
      "\n",
      "chat_format: null\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tmp/models/gemma-2b/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00002.safetensors\n",
      "  - model-00002-of-00002.safetensors\n",
      "  model_type: GEMMA\n",
      "  output_dir: ./models/\n",
      "  recipe_checkpoint: null\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_kv_cache: true\n",
      "max_new_tokens: 300\n",
      "model:\n",
      "  _component_: torchtune.models.gemma.gemma_2b\n",
      "prompt:\n",
      "  system: ''\n",
      "  user: 'What are the three primary colors?\n",
      "\n",
      "\n",
      "    '\n",
      "quantizer: null\n",
      "seed: 2\n",
      "temperature: 0.95\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.gemma.gemma_tokenizer\n",
      "  path: /tmp/models/gemma-2b/tokenizer.model\n",
      "  prompt_template:\n",
      "    system:\n",
      "    - Below is an instruction that describes a task. Write a response that appropriately\n",
      "      completes the request.\n",
      "    - '\n",
      "\n",
      "\n",
      "      '\n",
      "    user:\n",
      "    - '### Instruction:\n",
      "\n",
      "      '\n",
      "    - '### Response:\n",
      "\n",
      "      '\n",
      "top_k: 300\n",
      "\n",
      "Setting manual seed to local seed 2. Local seed is seed + rank = 2 + 0\n",
      "Model is initialized with precision torch.bfloat16.\n"
     ]
    }
   ],
   "source": [
    "!tune run generate --config ./config_files/custom_generate_gemma2-2b.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... it's a little unhinged and the response to our instruction was not direct or succint. If you want to change the prompt being sent to the model, you can add the command line override \n",
    "\n",
    "`prompt.user=\"Your new prompt\" `\n",
    "\n",
    "to the command above. You can get different responses to the same prompt by chaning the seed value, and higher temperature values will make the model output more random.  Now let's see how the fine-tuned model performs with the same prompt.\n",
    "\n",
    "<h2><center>1.2 Prompting the fine-tuned model</center></h2>\n",
    "The command below will send the same prompt to the fine-tuned model. We can reuse the configuration file that we used for the base model, and just override the checkpoint directory and file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-22:09:35:19,138 INFO     [_logging.py:101] Running InferenceRecipe with resolved config:\n",
      "\n",
      "chat_format: null\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /work2/10156/gj3385/frontera/ml_institute/models/gemma-2b-lora-finetuned-alpaca/\n",
      "  checkpoint_files:\n",
      "  - hf_model_0001_0.pt\n",
      "  - hf_model_0002_0.pt\n",
      "  model_type: GEMMA\n",
      "  output_dir: ./models/\n",
      "  recipe_checkpoint: null\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_kv_cache: true\n",
      "max_new_tokens: 300\n",
      "model:\n",
      "  _component_: torchtune.models.gemma.gemma_2b\n",
      "prompt:\n",
      "  system: ''\n",
      "  user: 'What are the three primary colors?\n",
      "\n",
      "\n",
      "    '\n",
      "quantizer: null\n",
      "seed: 2\n",
      "temperature: 0.95\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.gemma.gemma_tokenizer\n",
      "  path: /work2/10156/gj3385/frontera/ml_institute/models/gemma-2b/tokenizer.model\n",
      "  prompt_template:\n",
      "    system:\n",
      "    - Below is an instruction that describes a task. Write a response that appropriately\n",
      "      completes the request.\n",
      "    - '\n",
      "\n",
      "\n",
      "      '\n",
      "    user:\n",
      "    - '### Instruction:\n",
      "\n",
      "      '\n",
      "    - '### Response:\n",
      "\n",
      "      '\n",
      "top_k: 300\n",
      "\n",
      "2024-10-22:09:35:19,247 DEBUG    [seed.py:60] Setting manual seed to local seed 2. Local seed is seed + rank = 2 + 0\n",
      "2024-10-22:09:35:22,513 INFO     [generate.py:81] Model is initialized with precision torch.bfloat16.\n",
      "2024-10-22:09:35:24,272 INFO     [generate.py:177] Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the three primary colors?\n",
      "\n",
      "### Response:\n",
      "Below are the three primary colors: red, green, and blue.\n",
      "2024-10-22:09:35:24,273 INFO     [generate.py:190] Time for inference: 1.53 sec total, 10.43 tokens/sec\n",
      "2024-10-22:09:35:24,273 INFO     [generate.py:193] Bandwidth achieved: 66.45 GB/s\n",
      "2024-10-22:09:35:24,274 INFO     [generate.py:194] Memory used: 6.66 GB\n"
     ]
    }
   ],
   "source": [
    "!tune run generate --config ./config_files/custom_generate_gemma2-2b.yaml \\\n",
    "checkpointer.checkpoint_dir=/tmp/models/gemma-2b-lora-finetuned-alpaca/ \\\n",
    "checkpointer.checkpoint_files=[hf_model_0001_0.pt,hf_model_0002_0.pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned model is *almost* correct, it at least gave us 2/3 correct colors and appears to be more succint in its responses. The brevity of the fine-tuned model response is perhaps not surprising given that the *alapaca* training dataset consists of relatively short question and answer queries so our lora likely has conditioned the model towards shorter responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Benchmarking the model output\n",
    "Manually coming up with prompts for the model is fun, but benchmarks will provide us with a more quantitative way to gauge any improvement in our model and compare its capabilities to other models.  Torchtune integrates with <a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">EleutherAI’s evaluation harness</a> which provides access to many different generation benchmarks.  As an example, we can use the <a href=\"https://github.com/sylinrl/TruthfulQA\">TruthfulQA</a> benchmark which evaluates the model's ability to generate factually correct answers to a question. There are several different testing methods offered in this benchmark, in our case we will be running MC2 in which the model attempts to chose which of the multiple choice responses are true given a particular question. Note, the benchmark takes ~20 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-22:09:35:50,279 INFO     [_logging.py:101] Running EleutherEvalRecipe with resolved config:\n",
      "\n",
      "batch_size: 8\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /work2/10156/gj3385/frontera/ml_institute/models/gemma-2b/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00002.safetensors\n",
      "  - model-00002-of-00002.safetensors\n",
      "  model_type: GEMMA\n",
      "  output_dir: ./models/\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_kv_cache: true\n",
      "limit: null\n",
      "max_seq_length: 4096\n",
      "model:\n",
      "  _component_: torchtune.models.gemma.gemma_2b\n",
      "quantizer: null\n",
      "seed: 1234\n",
      "tasks:\n",
      "- truthfulqa_mc2\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.gemma.gemma_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: /work2/10156/gj3385/frontera/ml_institute/models/gemma-2b/tokenizer.model\n",
      "\n",
      "2024-10-22:09:35:50,397 DEBUG    [seed.py:60] Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
      "2024-10-22:09:35:53,658 INFO     [eleuther_eval.py:240] Model is initialized with precision torch.bfloat16.\n",
      "2024-10-22:09:35:53,779 INFO     [eleuther_eval.py:216] Tokenizer is initialized from file.\n",
      "2024-10-22:09:35:53,893 WARNING  [other.py:349] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-10-22:09:35:53,893 INFO     [huggingface.py:130] Using device 'cuda:0'\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-10-22:09:35:54,462 INFO     [huggingface.py:366] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "2024-10-22:09:35:57,829 INFO     [__init__.py:491] `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.\n",
      "2024-10-22:09:36:11,276 INFO     [eleuther_eval.py:266] Running evaluation on ['truthfulqa_mc2'] tasks.\n",
      "2024-10-22:09:36:11,280 INFO     [task.py:423] Building contexts for truthfulqa_mc2 on rank 0...\n",
      "100%|████████████████████████████████████████| 817/817 [00:01<00:00, 703.01it/s]\n",
      "2024-10-22:09:36:12,518 INFO     [evaluator.py:465] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|███████| 5882/5882 [17:47<00:00,  5.51it/s]\n",
      "2024-10-22:09:54:05,330 INFO     [eleuther_eval.py:273] Eval completed in 1091.49 seconds.\n",
      "|    Tasks     |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|--------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|truthfulqa_mc2|      2|none  |     0|acc   |↑  |0.3993|±  |0.0152|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tune run eleuther_eval --config ./config_files/custom_eval_gemma2-2b.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my test, the base model got a score of 0.3993 ± 0.0152. We can run the same evaluation on the fine-tuned model using the command below. Again we'll reuse the configuration file and just point to our fine-tuned model checkpoint directory and files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-22:10:21:10,199 INFO     [_logging.py:101] Running EleutherEvalRecipe with resolved config:\n",
      "\n",
      "batch_size: 8\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /work2/10156/gj3385/frontera/ml_institute/models/gemma-2b-lora-finetuned-alpaca/\n",
      "  checkpoint_files:\n",
      "  - hf_model_0001_0.pt\n",
      "  - hf_model_0002_0.pt\n",
      "  model_type: GEMMA\n",
      "  output_dir: ./models/\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_kv_cache: true\n",
      "limit: null\n",
      "max_seq_length: 4096\n",
      "model:\n",
      "  _component_: torchtune.models.gemma.gemma_2b\n",
      "quantizer: null\n",
      "seed: 1234\n",
      "tasks:\n",
      "- truthfulqa_mc2\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.gemma.gemma_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: /work2/10156/gj3385/frontera/ml_institute/models/gemma-2b/tokenizer.model\n",
      "\n",
      "2024-10-22:10:21:10,352 DEBUG    [seed.py:60] Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
      "2024-10-22:10:21:22,643 INFO     [eleuther_eval.py:240] Model is initialized with precision torch.bfloat16.\n",
      "2024-10-22:10:21:22,771 INFO     [eleuther_eval.py:216] Tokenizer is initialized from file.\n",
      "2024-10-22:10:21:22,909 WARNING  [other.py:349] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-10-22:10:21:22,909 INFO     [huggingface.py:130] Using device 'cuda:0'\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-10-22:10:21:23,277 INFO     [huggingface.py:366] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "2024-10-22:10:21:26,803 INFO     [__init__.py:491] `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.\n",
      "2024-10-22:10:21:38,972 INFO     [eleuther_eval.py:266] Running evaluation on ['truthfulqa_mc2'] tasks.\n",
      "2024-10-22:10:21:38,976 INFO     [task.py:423] Building contexts for truthfulqa_mc2 on rank 0...\n",
      "100%|████████████████████████████████████████| 817/817 [00:01<00:00, 689.01it/s]\n",
      "2024-10-22:10:21:40,232 INFO     [evaluator.py:465] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|███████| 5882/5882 [17:00<00:00,  5.76it/s]\n",
      "2024-10-22:10:38:46,117 INFO     [eleuther_eval.py:273] Eval completed in 1043.25 seconds.\n",
      "|    Tasks     |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|--------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|truthfulqa_mc2|      2|none  |     0|acc   |↑  |0.3953|±  |0.0152|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tune run eleuther_eval --config ./config_files/custom_eval_gemma2-2b.yaml \\\n",
    "checkpointer.checkpoint_dir=/tmp/models/gemma-2b-lora-finetuned-alpaca/ \\\n",
    "checkpointer.checkpoint_files=[hf_model_0001_0.pt,hf_model_0002_0.pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned model gets a score of 0.3953 ± 0.0152 which is not statistically different from the base model. There are several explanations for this.  First, the fine-tuned model was only trained on the dataset for 1 epoch, so training for longer *might* further influence its score.  The second and more likely explanation is explained in the paper published along with the benchmark (https://arxiv.org/abs/2109.07958).\n",
    "\n",
    "\n",
    "\"Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.\"\n",
    "\n",
    "The alpaca dataset we trained our model on was generated by an earlier version of Chat GPT (see https://huggingface.co/datasets/tatsu-lab/alpaca) which means the dataset itself is likely to contain many false and incorrect answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Tutorial-March-2025",
   "language": "python",
   "name": "jupyter_kernel_config"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
