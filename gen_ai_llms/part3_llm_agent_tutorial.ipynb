{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook we demonstrate some useful ways LLM's can be employed beyond simple question and answering tasks. We will show how to use LLMs to:\n",
    "\n",
    "+ write api calls to trigger other software (tool calls)\n",
    "+ break down multi-step problems into multiple smaller steps (goal-decomposition)\n",
    "+ categorize an input to a set of pre-defined labels.\n",
    "\n",
    "Finally, we will combine these features to create a simple AI agent capable of autonomously tackling multi-step problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "We have already set up a container with the required depencies for you to run everything in this module.  I've included dependency installation instructions here in case you want to get this notebook running on your own setup.  You will need to run a local ollama server (https://ollama.com/) for LLM inference and install the ollama python package (https://github.com/ollama/ollama-python).  Note that the pip install of the ollama package below only includes a python integration for ollama, you still need to download and install the ollama backend from their website.\n",
    "\n",
    "1) Install ollama:\n",
    "\n",
    "    pip install ollama\n",
    "\n",
    "2) Start an ollama server by running the command below in a terminal. Note, there's a method below to run the server in the background of this notebook but at home I'd recommend running the server in a separate terminal and not in a notebook so you can see the server status more easily.\n",
    "\n",
    "    ollama serve\n",
    "\n",
    "Ollama should download models automatically whenever they are requested, but to force it to get the model I'm using below you can always run the following command in a new terminal once the server is up.\n",
    "\n",
    "    ollama run llama3.2\n",
    "\n",
    "Not all models are able to accept tool calls, when browsing the models library (https://ollama.com/library), the models with the **tools** icon under their name will accept tool calls.\n",
    "\n",
    "You will also need the duckduckgo search python library to allow our agent to browse the web\n",
    "\n",
    "    pip install -U duckduckgo_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Tools\n",
    "### Tool Transcriber\n",
    "This function converts a function's python code into a dictionary object that contains a description of what the function does and defines its inputs and outputs so that the LLM will understand how to execute it. This code is copied from here https://github.com/meirm/ollama-tools.\n",
    "\n",
    "When querying the LLM to write a tool call we will use the function **llm_prompt_tool()** which requires a prompt as well as a list of the available tools the model can use. We can use any python function we want as a tool. To create this list of tools to pass to the **llm_prompt_tool()** function, you just need to run \n",
    "\n",
    "    tools = [generate_function_description(<function 1 name>),\n",
    "             generate_function_description(<function 2 name>),\n",
    "             ...\n",
    "             ]\n",
    "\n",
    "You then use this tools object to prompt the LLM like\n",
    "\n",
    "    tool_output = llm_prompt_tool(prompt='your prompt', tools=tools)\n",
    "\n",
    "This would send the LLM your prompt and list of tools, the LLM will respond with the tool it wants to execute and the input arguments, and then we execute the tool function and return the function's output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json\n",
    "import re\n",
    "def generate_function_description(func):\n",
    "    func_name = func.__name__\n",
    "    docstring = func.__doc__\n",
    "\n",
    "    # Get function signature\n",
    "    sig = inspect.signature(func)\n",
    "    params = sig.parameters\n",
    "\n",
    "    # Create the properties for parameters\n",
    "    properties = {}\n",
    "    required = []\n",
    "\n",
    "    # Process the docstring to extract argument descriptions\n",
    "    arg_descriptions = {}\n",
    "    if docstring:\n",
    "        # remove leading/trailing whitespace or leading empty lines and split into lines\n",
    "        docstring = re.sub(r'^\\s*|\\s*$', '', docstring, flags=re.MULTILINE)\n",
    "        lines = docstring.split('\\n')\n",
    "        current_arg = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                if ':' in line:\n",
    "                    # strip leading/trailing whitespace and split into two parts\n",
    "                    line = re.sub(r'^\\s*|\\s*$', '', line)\n",
    "                    parts = line.split(':', 1)\n",
    "                    if parts[0] in params:\n",
    "                        current_arg = parts[0]\n",
    "                        arg_descriptions[current_arg] = parts[1].strip()\n",
    "                elif current_arg:\n",
    "                    arg_descriptions[current_arg] += ' ' + line.strip()\n",
    "\n",
    "    for param_name, param in params.items():\n",
    "        param_type = 'string'  # Default type; adjust as needed based on annotations\n",
    "        if param.annotation != inspect.Parameter.empty:\n",
    "            param_type = param.annotation.__name__.lower()\n",
    "\n",
    "        param_description = arg_descriptions.get(param_name, f'The name of the {param_name}')\n",
    "\n",
    "        properties[param_name] = {\n",
    "            'type': param_type,\n",
    "            'description': param_description,\n",
    "        }\n",
    "        if param.default == inspect.Parameter.empty:\n",
    "            required.append(param_name)\n",
    "\n",
    "    # Create the JSON object\n",
    "    function_description = {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': func_name,\n",
    "            'description': docstring.split('\\n')[0] if docstring else f'Function {func_name}',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': properties,\n",
    "                'required': required,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return function_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tool functions\n",
    "These are the functions that we'll set up as tools for the AI agent. Note that each function definition below has a multi-line docstring that describes its purpose, inputs, and outputs.  These are what **generate_function_description()** uses to make our tool description for the LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "\n",
    "def get_duckduckgo_result(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the top DuckDuckGo search result for the given query.\n",
    "    query: The search query to send to DuckDuckGo.\n",
    "    \"\"\"\n",
    "    results = DDGS().text(query, \n",
    "                          region='wt-wt', \n",
    "                          safesearch='off', \n",
    "                          timelimit='y', \n",
    "                          max_results=10)\n",
    "    \n",
    "    return results[0]['body']\n",
    "\n",
    "def do_math(a:int, op:str, b:int)->list:\n",
    "    \"\"\"\n",
    "    Performs math on the inputs\n",
    "    a: The first operand\n",
    "    op: The operation to perform (one of '+', '-', '*', '/')\n",
    "    b: The second operand\n",
    "    \"\"\"\n",
    "    res = \"Nan\"\n",
    "    if op == \"+\":\n",
    "        res = str(int(a) + int(b))\n",
    "    elif op == \"-\":\n",
    "        res = str(int(a) - int(b))\n",
    "    elif op == \"*\":\n",
    "        res = str(int(a) * int(b))\n",
    "    elif op == \"/\":\n",
    "        if int(b) != 0:\n",
    "            res = str(int(a) / int(b))\n",
    "    return res\n",
    "\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current time\"\"\"\n",
    "    current_time = time.strftime(\"%H:%M:%S\")\n",
    "    return f\"The current time is {current_time}\"\n",
    "\n",
    "def get_current_weather(city:str) -> str:\n",
    "    \"\"\"Get the current weather for a city\n",
    "    Args:\n",
    "        city: The city to get the weather for\n",
    "    \"\"\"\n",
    "    base_url = f\"http://wttr.in/{city}?format=j1\"\n",
    "    response = requests.get(base_url)\n",
    "    data = response.json()\n",
    "    return f\"The current temperature in {city} is: {data['current_condition'][0]['temp_C']}Â°C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building tools object example\n",
    "Here is an example that builds the stringified tools data to send to our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stringified tool descriptions:\n",
      "[\n",
      "    {\n",
      "        \"type\": \"function\",\n",
      "        \"function\": {\n",
      "            \"name\": \"get_duckduckgo_result\",\n",
      "            \"description\": \"Get the top DuckDuckGo search result for the given query.\",\n",
      "            \"parameters\": {\n",
      "                \"type\": \"object\",\n",
      "                \"properties\": {\n",
      "                    \"query\": {\n",
      "                        \"type\": \"str\",\n",
      "                        \"description\": \"The search query to send to DuckDuckGo.\"\n",
      "                    }\n",
      "                },\n",
      "                \"required\": [\n",
      "                    \"query\"\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"function\",\n",
      "        \"function\": {\n",
      "            \"name\": \"do_math\",\n",
      "            \"description\": \"Performs math on the inputs\",\n",
      "            \"parameters\": {\n",
      "                \"type\": \"object\",\n",
      "                \"properties\": {\n",
      "                    \"a\": {\n",
      "                        \"type\": \"int\",\n",
      "                        \"description\": \"The first operand\"\n",
      "                    },\n",
      "                    \"op\": {\n",
      "                        \"type\": \"str\",\n",
      "                        \"description\": \"The operation to perform (one of '+', '-', '*', '/')\"\n",
      "                    },\n",
      "                    \"b\": {\n",
      "                        \"type\": \"int\",\n",
      "                        \"description\": \"The second operand\"\n",
      "                    }\n",
      "                },\n",
      "                \"required\": [\n",
      "                    \"a\",\n",
      "                    \"op\",\n",
      "                    \"b\"\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "tools = [\n",
    "    generate_function_description(get_duckduckgo_result),\n",
    "    generate_function_description(do_math)]\n",
    "\n",
    "print(f\"Stringified tool descriptions:\\n{json.dumps(tools, indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Function: llm_prompt(prompt, system_message, seed) -> str\n",
    "This function sends the prompt to the LLM and returns the response text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Sends the prompt to the LLM and returns the message response string\n",
    "def llm_prompt(prompt: str, \n",
    "               system_message: str=\"You are a helpful AI Agent named Kiwi.\", \n",
    "               seed: int=-1, \n",
    "               model: str=\"llama3.2\") -> str:\n",
    "\n",
    "        # generate a text response by sending our prompt to the ollama server \n",
    "        response = ollama.chat(\n",
    "            model=\"llama3.2\", \n",
    "            options={\"seed\":seed}, \n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        return response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Function: llm_prompt_tool(prompt, tools, system_message, seed) -> str\n",
    "This function sends the prompt and a list of tools to the LLM, executes the tool call the LLM generates and returns the result. Note, it only executes the first tool call the LLM wants to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Sends the prompt to the LLM and returns the message response string\n",
    "def llm_prompt_tool(prompt: str, \n",
    "               tools: list,\n",
    "               system_message: str=\"You are a helpful AI Agent named Kiwi.\", \n",
    "               seed: int=-1, \n",
    "               model: str=\"llama3.2\") -> str:\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=\"llama3.2\", \n",
    "            options={\"seed\":seed}, \n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},{\"role\": \"user\", \"content\": prompt}],\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "        # pull out the tool call dictionary from the response object\n",
    "        tools_calls = response['message']['tool_calls']\n",
    "        print(f\"Tool call:\\n{tools_calls}\\n\") \n",
    "    \n",
    "        # get the name and input arguments of the tool\n",
    "        tool_name = tools_calls[0]['function']['name']\n",
    "        arguments = tools_calls[0]['function']['arguments']\n",
    "\n",
    "        # execute the tool\n",
    "        result = globals()[tool_name](**arguments)\n",
    "    \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Function: llm_create_list(prompt) -> dict\n",
    "This function queries the LLM with prompt and has it return a json-formatted list with descriptions of each list item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# formats the prompt into a template string that asks for list in response\n",
    "def create_list_template(prompt: str) -> str:\n",
    "\n",
    "    list_schema = \"\"\"```json\n",
    "{\n",
    "\"list_description\": \"Describe list contents here\",\n",
    "\"content\":[\n",
    "{\"name\": \"Name of list item 1\", \"description\": \"Description of list item 1\"},\n",
    "{\"name\": \"Name of list item 2\", \"description\": \"Description of list item 2\"},\n",
    "...\n",
    "]\n",
    "}\n",
    "```\"\"\"\n",
    "    \n",
    "    output_prompt = f\"\"\"The following prompt requires you to respond with a list.\n",
    "\n",
    "*Prompt:* {prompt}\n",
    "\n",
    "*List format schema:*\n",
    "{list_schema}\n",
    "\n",
    "The \"...\" indicates that the list can be as many items long as you require.\n",
    "\n",
    "Respond with the list that address the prompt using the above formatting schema.\"\"\"\n",
    "    \n",
    "    return output_prompt\n",
    "\n",
    "# Given a prompt, ask the LLM to make a list in response, retry until it responds in a good format\n",
    "# Will respond with a dictionary with a list 'description' and 'content' which is an array of dicts\n",
    "# with 'name' and 'description' of each list item.  An empty dict is returned if it fails to make\n",
    "# a well formatted list\n",
    "def llm_create_list(prompt: str) -> dict:\n",
    "\n",
    "    # Desired system message\n",
    "    system_message = \"You are a helpful AI Agent named Kiwi.\"\n",
    "\n",
    "    # Embed our prompt into the pick_option_template\n",
    "    prompt = create_list_template(prompt)\n",
    "    \n",
    "    # # For testing, print templated prompt\n",
    "    # print(f\"Templated Prompt:\\n{prompt}\")\n",
    "\n",
    "    # set the choice to no choice selected\n",
    "    generated_list = {}\n",
    "    \n",
    "    # We'll try 10 times to get a valid choice\n",
    "    for seed in range(1, 10):\n",
    "\n",
    "        # # For testing, print current seed value\n",
    "        # print(f\"Querying model with random seed vaule {seed}...\\n\")\n",
    "\n",
    "        # print progress\n",
    "        status = f\"trying to create list [{seed}/10] times...\" \n",
    "        print(status,end='')\n",
    "        \n",
    "        # Send our pick tools prompt to the model\n",
    "        response_text = llm_prompt(prompt, system_message)\n",
    "        \n",
    "        # try to convert the LLM response to an int\n",
    "        try:\n",
    "\n",
    "            # find the json data in the response which should start with ```json and end with ```\n",
    "            start_index = response_text.find(\"```json\")+7\n",
    "            end_index = response_text.find(\"```\", start_index)\n",
    "\n",
    "            # pull out just the json data\n",
    "            cleaned_list_string = response_text[start_index:end_index]\n",
    "\n",
    "            # load into a json object\n",
    "            generated_list = json.loads(cleaned_list_string)\n",
    "            \n",
    "            # Display success message\n",
    "            print(f\"success! :D\\n\")\n",
    "\n",
    "            # print(f\"Cleaned LLM Response:\\n{cleaned_list_string}\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred parsing the generated list json:\\n {e}\")\n",
    "            # For testing, print response\n",
    "            # print(f\"Raw LLM Response:\\n{response_text}\\n\")\n",
    "            # print(f\"\\n\\n Cleaned LLM Response:\\n{response_text.strip('```json').split('```')[0].strip()}\")\n",
    "        \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LLM Function: llm_pick_option(prompt, options, none_option) -> int\n",
    "This function queries the LLM repeatedly until it chooses one of the provided options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Takes a list of dictionary objects with the 'name' and 'description'\n",
    "# and builds a string with a numbered list. The none_option flag \n",
    "# inserts an \"Option 0: None of these\" into the choices\n",
    "def build_option_list(options: list,none_option: bool=False) -> str:\n",
    "\n",
    "    # add an option to selection none\n",
    "    if none_option:\n",
    "        output_string = \"Option 0: None of these\\n\\n\"\n",
    "        i_offset = 1\n",
    "    else:\n",
    "        output_string = \"\"\n",
    "        i_offset = 0\n",
    "\n",
    "    # build the rest of the options from the input options list\n",
    "    output_string += \"\\n\\n\".join(f\"Option {i+i_offset}: {option.get('name')} to {option.get('description')}\" for i, option in enumerate(options))\n",
    "\n",
    "    return output_string\n",
    "\n",
    "# formats the prompt and available options to choose from into a template string\n",
    "def pick_option_template(prompt: str, options: str) -> str:\n",
    "\n",
    "    output_prompt = f\"\"\"Choose which option is best suited to address the prompt.\n",
    "\n",
    "*Prompt:* {prompt}\n",
    "\n",
    "*Options:*\\n{options}\n",
    "\n",
    "Respond with a single number.\"\"\"\n",
    "    \n",
    "    return output_prompt\n",
    "\n",
    "# Asks the LLM which option (array of dictionaries with 'name' and 'description') to use to answer the prompt\n",
    "# returns the LLM's choice as a single integer\n",
    "# if non_option = True, choice = 0 means \"None of these\", otherwise 0 means the first option you provided.\n",
    "# A choice = -1 means the model could not make a valid choice (it will try 100 seeds by default before giving up)\n",
    "def llm_pick_option(prompt: str, options: list, none_option: bool) -> int:\n",
    "\n",
    "    # Desired system message\n",
    "    system_message = \"You are a helpful AI Agent named Kiwi.\"\n",
    "\n",
    "    # Convert to a string that contains the list of options\n",
    "    options_list = build_option_list(options, none_option)\n",
    "\n",
    "    # For testing, print out the options string we will send\n",
    "    # print(\"Options List:\\n\\n\" + options_list + \"\\n\")\n",
    "\n",
    "    # Embed our prompt into the pick_option_template\n",
    "    prompt = pick_option_template(prompt, options_list)\n",
    "    \n",
    "    # For testing, print templated prompt\n",
    "    # print(f\"Templated Prompt:\\n{prompt}\")\n",
    "\n",
    "    # set the choice to no choice selected\n",
    "    choice = -1\n",
    "    \n",
    "    # We'll try 100 times to get a valid choice\n",
    "    for seed in range(1, 101):\n",
    "\n",
    "        # For testing, print current seed value\n",
    "        # print(f\"Querying model with random seed vaule {seed}...\\n\")\n",
    "\n",
    "        # Send our pick tools prompt to the model\n",
    "        response_text = llm_prompt(prompt, system_message)\n",
    "        \n",
    "        # try to convert the LLM response to an int\n",
    "        try:\n",
    "            choice = int(response_text)\n",
    "\n",
    "            # For testing, displauy chosen option\n",
    "            # print(f\"Chose option {choice}!\")\n",
    "            break\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "            # For testing, display that it failed to make a choice\n",
    "            # print('Failed to convert response to int :/')\n",
    "        \n",
    "    return choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting LLM backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting ollama server in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/04 14:35:02 routes.go:1205: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home1/10386/lsmith9003/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2025-03-04T14:35:02.450-06:00 level=INFO source=images.go:432 msg=\"total blobs: 6\"\n",
      "time=2025-03-04T14:35:02.451-06:00 level=INFO source=images.go:439 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-03-04T14:35:02.454-06:00 level=INFO source=routes.go:1256 msg=\"Listening on 127.0.0.1:11434 (version 0.5.12)\"\n",
      "time=2025-03-04T14:35:02.456-06:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
      "time=2025-03-04T14:35:03.062-06:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-cc7e93c2-5e50-937f-9850-671ad8808b8b library=cuda variant=v12 compute=7.5 driver=12.2 name=\"Quadro RTX 5000\" total=\"15.7 GiB\" available=\"15.6 GiB\"\n",
      "time=2025-03-04T14:35:03.062-06:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-4407418d-3cc9-b7ed-fde4-54abd1857b6f library=cuda variant=v12 compute=7.5 driver=12.2 name=\"Quadro RTX 5000\" total=\"15.7 GiB\" available=\"15.6 GiB\"\n",
      "time=2025-03-04T14:35:03.062-06:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-66469920-cfbb-e70f-48b4-d1a33bb4beb4 library=cuda variant=v12 compute=7.5 driver=12.2 name=\"Quadro RTX 5000\" total=\"15.7 GiB\" available=\"15.6 GiB\"\n",
      "time=2025-03-04T14:35:03.062-06:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-579ac947-fc5f-1ec0-2a13-ce15a8d7922d library=cuda variant=v12 compute=7.5 driver=12.2 name=\"Quadro RTX 5000\" total=\"15.7 GiB\" available=\"15.6 GiB\"\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def run_ollama():\n",
    "    subprocess.run(\"ollama serve\", shell=True)\n",
    "\n",
    "ollama_thread = threading.Thread(target=run_ollama)\n",
    "ollama_thread.start()\n",
    "\n",
    "# Give Ollama some time to start up\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the llama3.2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/03/04 - 14:35:12 | 200 |     106.839Âµs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2025/03/04 - 14:35:12 | 200 |   222.72906ms |       127.0.0.1 | POST     \"/api/show\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ´ \u001b[K\u001b[?25h\u001b[?2026ltime=2025-03-04T14:35:12.981-06:00 level=INFO source=sched.go:715 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/home1/10386/lsmith9003/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-cc7e93c2-5e50-937f-9850-671ad8808b8b parallel=4 available=16775512064 required=\"3.7 GiB\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ § \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026ltime=2025-03-04T14:35:13.387-06:00 level=INFO source=server.go:97 msg=\"system memory\" total=\"125.6 GiB\" free=\"117.8 GiB\" free_swap=\"0 B\"\n",
      "time=2025-03-04T14:35:13.387-06:00 level=INFO source=server.go:130 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split=\"\" memory.available=\"[15.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"3.7 GiB\" memory.required.partial=\"3.7 GiB\" memory.required.kv=\"896.0 MiB\" memory.required.allocations=\"[3.7 GiB]\" memory.weights.total=\"2.4 GiB\" memory.weights.repeating=\"2.1 GiB\" memory.weights.nonrepeating=\"308.2 MiB\" memory.graph.full=\"424.0 MiB\" memory.graph.partial=\"570.7 MiB\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026ltime=2025-03-04T14:35:13.464-06:00 level=INFO source=server.go:380 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /home1/10386/lsmith9003/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 16 --parallel 4 --port 45418\"\n",
      "time=2025-03-04T14:35:13.465-06:00 level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\n",
      "time=2025-03-04T14:35:13.465-06:00 level=INFO source=server.go:557 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-03-04T14:35:13.466-06:00 level=INFO source=server.go:591 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
      "time=2025-03-04T14:35:13.484-06:00 level=INFO source=runner.go:932 msg=\"starting go runner\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ´ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ § \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026lggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Quadro RTX 5000, compute capability 7.5, VMM: yes\n",
      "load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ ¼ \u001b[K\u001b[?25h\u001b[?2026lload_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so\n",
      "time=2025-03-04T14:35:14.820-06:00 level=INFO source=runner.go:935 msg=system info=\"CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | cgo(gcc)\" threads=16\n",
      "time=2025-03-04T14:35:14.820-06:00 level=INFO source=runner.go:993 msg=\"Server listening on 127.0.0.1:45418\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ ´ \u001b[K\u001b[?25h\u001b[?2026lllama_load_model_from_file: using device CUDA0 (Quadro RTX 5000) - 15998 MiB free\n",
      "time=2025-03-04T14:35:14.975-06:00 level=INFO source=server.go:591 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ ¦ \u001b[K\u001b[?25h\u001b[?2026lllama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home1/10386/lsmith9003/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ § \u001b[K\u001b[?25h\u001b[?2026lllama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026lllm_load_vocab: special tokens cache size = 256\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026lllm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 24\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 3\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.21 B\n",
      "llm_load_print_meta: model size       = 1.87 GiB (5.01 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ã'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ ¼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ´ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ § \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ´ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ § \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026lllm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =   308.23 MiB\n",
      "llm_load_tensors:        CUDA0 model buffer size =  1918.35 MiB\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ ¼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ´ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ § \u001b[K\u001b[?25h\u001b[?2026lllama_new_context_with_model: n_seq_max     = 4\n",
      "llama_new_context_with_model: n_ctx         = 8192\n",
      "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
      "llama_new_context_with_model: n_batch       = 2048\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   896.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.00 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   424.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 902\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026ltime=2025-03-04T14:35:18.238-06:00 level=INFO source=server.go:596 msg=\"llama runner started in 4.77 seconds\"\n",
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/03/04 - 14:35:18 | 200 |  5.839675963s |       127.0.0.1 | POST     \"/api/generate\"\n"
     ]
    }
   ],
   "source": [
    "def download_model():\n",
    "    subprocess.run(\"ollama run llama3.2\", shell=True)\n",
    "\n",
    "model_download_thread = threading.Thread(target=download_model)\n",
    "model_download_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check ollama server status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/03/04 - 14:35:28 | 200 |      25.266Âµs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2025/03/04 - 14:35:28 | 200 |     183.677Âµs |       127.0.0.1 | GET      \"/api/ps\"\n",
      "NAME               ID              SIZE      PROCESSOR    UNTIL              \r\n",
      "llama3.2:latest    a80c4f17acd5    4.0 GB    100% GPU     4 minutes from now    \r\n"
     ]
    }
   ],
   "source": [
    "! ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close ollama server\n",
    "If you want to close the server, run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "! kill $(pgrep ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Function Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example sending simple prompt\n",
    "The following example sends the *user_prompt* to our llm and then prints its response to the console.  The default model is set to be llama3.2, if you'd like to change it, you can add the optional input argument model=\"model name\" to the **llm_prompt()** function like:\n",
    "\n",
    "    response_text = llm_prompt(user_prompt,model=\"qwen:0.5b\")\n",
    "\n",
    "The available models are listed at https://ollama.com/library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home1/10386/lsmith9003/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 1\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = all F32\n",
      "llm_load_print_meta: model params     = 3.21 B\n",
      "llm_load_print_meta: model size       = 1.87 GiB (5.01 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ã'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llama_model_load: vocab only - skipping tensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/03/04 - 14:35:35 | 200 |  2.592167805s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "LLM Response:\n",
      "Twisted, climbing high\n",
      "Vines entwine with gentle touch\n",
      "Summer's gentle kiss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# user prompt\n",
    "user_prompt = \"Write a haiku about vines\"\n",
    "\n",
    "response_text = llm_prompt(user_prompt)\n",
    "\n",
    "print(f\"LLM Response:\\n{response_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of asking the LLM to pick between options\n",
    "In this example we will ask the LLM to decide which option (in this case, each option is a description of one of our tools) is best suited to address the given prompt. The *none_option=True* means that a response of 0 will mean the LLM chose \"none of the above\", and then numbers 1-N correspond to a choice of each of our N tools. Our *user_prompt* is clearly outlining a math problem, so we should expect the LLM to chose option 3 which corresponds to our **do_math** function.  For redundancy, we'll have the model chose several times to see if it's option choice varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/03/04 - 14:35:42 | 200 |  957.969379ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:42 | 200 |  170.432633ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:43 | 200 |  384.092409ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:43 | 200 |  452.705359ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:43 | 200 |  284.075433ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:44 | 200 |  396.122293ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:44 | 200 |  150.556937ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:44 | 200 |  105.244762ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:44 | 200 |  304.161716ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:45 | 200 |  105.928413ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:45 | 200 |  543.768465ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:45 | 200 |  340.557369ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:46 | 200 |   460.49435ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:47 | 200 |  790.971141ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:47 | 200 |   79.330481ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:47 | 200 |  125.945148ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:47 | 200 |  104.572795ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:47 | 200 |  168.058148ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:47 | 200 |  104.220278ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:47 | 200 |  104.452143ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:47 | 200 |    125.4714ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:48 | 200 |  492.897957ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:48 | 200 |  158.712569ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:48 | 200 |  261.464849ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:48 | 200 |  104.503513ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:49 | 200 |  104.340632ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:49 | 200 |  179.123587ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:49 | 200 |  124.252366ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:49 | 200 |  383.635381ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:49 | 200 |  143.929371ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:49 | 200 |   55.322041ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:50 | 200 |  397.899559ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:50 | 200 |  351.370159ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:50 | 200 |  126.662228ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:51 | 200 |  161.795403ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:51 | 200 |  106.699815ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:51 | 200 |   63.840308ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:51 | 200 |  103.574678ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:51 | 200 |  485.108123ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:51 | 200 |  103.244233ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/03/04 - 14:35:51 | 200 |   84.375102ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "The LLM chose options [3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "# user prompt\n",
    "user_prompt = \"Multiply 626183 with 182731\"\n",
    "\n",
    "# For testing, generate function descriptions\n",
    "function_descriptions = [\n",
    "    generate_function_description(get_current_weather),\n",
    "    generate_function_description(get_current_time),\n",
    "    generate_function_description(do_math),\n",
    "    generate_function_description(get_duckduckgo_result)\n",
    "]       \n",
    "\n",
    "# Pull out the name and description dictionaries from our tools json data\n",
    "func_data = [func['function'] for func in function_descriptions]\n",
    "\n",
    "# Ask LLM to choose 3 times\n",
    "choices = [llm_pick_option(user_prompt, func_data, True) for i in range(0,3)]\n",
    "\n",
    "print(f\"The LLM chose options {choices}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of asking LLM to create a list\n",
    "In this example we'll prompt the LLM to create a step by step plan to accomplish a goal.  This can be very useful when trying to set up an AI agent to perform autonomously on complex multi-step problems. The **llm_create_list** functon prepends text to our prompt to instruct the LLM to respond in a specific format that we can parse into a json object. Note that the LLM often makes typos in its response which causes our fairly naive string parsing to fail.  We will try to get a well formated list 10 times before the **llm_create_list** function gives up.  Try changing the goal in the *user_prompt* string yourself to see what the LLM is capable of planning for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to create list [1/10] times...[GIN] 2025/03/04 - 14:36:03 | 200 |  4.527594282s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "success! :D\n",
      "\n",
      "Generated List:\n",
      "1. Identify the theme or topic\n",
      "2. Develop a unique perspective or voice\n",
      "3. Choose a suitable poetic form\n",
      "4. Create a compelling title\n",
      "5. Use sensory language and imagery\n",
      "6. Play with sound devices and rhythm\n",
      "7. Use figurative language effectively\n",
      "8. Edit and revise the poem\n",
      "9. Get feedback from others\n",
      "10. Finalize and polish the poem\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt for the model\n",
    "user_prompt = \"\"\"Break down the the following goal into subgoals\n",
    "\n",
    "Goal: Create a step by step guide to writing a compelling poem.\"\"\"\n",
    "\n",
    "# send the prompt to the LLM and have it return a list\n",
    "# The list should be a dict object with fields:\n",
    "#    {'list_description': 'Describe list contents here',\n",
    "#     'content':[{'name': 'Name of list item 1', 'description', 'Description of list item 1'},...]}\n",
    "generated_list = llm_create_list(user_prompt)\n",
    "\n",
    "# Check if list is empty, if not, print out the names of the list items\n",
    "if bool(generated_list):\n",
    "    # pull out list names\n",
    "    list_names = [item['name'] for item in generated_list['content']]\n",
    "\n",
    "    # print list to console for viewing\n",
    "    nl = \"\\n\"\n",
    "    print(f\"Generated List:\\n{nl.join([f'{i+1}. {name}' for i, name in enumerate(list_names)])}\\n\")\n",
    "else:\n",
    "    print(f\"No List generated :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to combine the functionality we've developed to create a simple AI agent!\n",
    "\n",
    "Usage: enter the goal for the AI Agent in the *user_prompt* and the agent will\n",
    "1) Create a step by step plan to achieve it based on the provided *tools*\n",
    "2) Execute each step sequentially by performing a tool call\n",
    "3) Concatenate all of the step results and format a final response\n",
    "\n",
    "Note that the agent will always write a tool call for every step, even if none of the tools available are applicable. In this case there isn't a tool for \"write poem\" so the agent will likely do something silly like **do_math** for a step that requires text generation.  Try changing the *user_prompt* to see what kinds of tasks the agent can handle! You can also add your own functions to the cell below.  Be sure to include a docstring defining what the funciton does and also add the function to the *tools* list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt: Add the temperature in Austin, TX to the temperature in Ann Arbor, MI and then write a poem whose rhyming scheme is based off the pronunciation of the combined temperatures\n",
      "\n",
      "Tools the LLM has access to:\n",
      "get_current_weather\n",
      "get_current_time\n",
      "do_math\n",
      "get_duckduckgo_result\n",
      "\n",
      "Generating step by step guide...\n",
      "trying to create list [1/10] times...[GIN] 2025/03/04 - 14:36:57 | 200 |    3.2448209s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "success! :D\n",
      "\n",
      "Generated Step by Step Guide:\n",
      "1. Get current temperature in Austin, TX\n",
      "2. Get current temperature in Ann Arbor, MI\n",
      "3. Add the temperatures of Austin and Ann Arbor\n",
      "4. Get current time in degrees Celsius\n",
      "5. Generate a poem based on the combined temperature's pronunciation\n",
      "\n",
      "Executing step [1/5]\n",
      "\n",
      "[GIN] 2025/03/04 - 14:36:57 | 200 |  376.573315ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "Tool call:\n",
      "[ToolCall(function=Function(name='get_current_weather', arguments={'city': 'Austin, TX'}))]\n",
      "\n",
      "Executing step [2/5]\n",
      "\n",
      "[GIN] 2025/03/04 - 14:36:58 | 200 |  389.743209ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "Tool call:\n",
      "[ToolCall(function=Function(name='get_current_weather', arguments={'city': 'Ann Arbor, MI'}))]\n",
      "\n",
      "Executing step [3/5]\n",
      "\n",
      "[GIN] 2025/03/04 - 14:36:59 | 200 |  275.171537ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "Tool call:\n",
      "[ToolCall(function=Function(name='do_math', arguments={'a': '18', 'b': '2', 'op': '+'}))]\n",
      "\n",
      "Executing step [4/5]\n",
      "\n",
      "[GIN] 2025/03/04 - 14:36:59 | 200 |  211.155131ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "Tool call:\n",
      "[ToolCall(function=Function(name='get_current_time', arguments={}))]\n",
      "\n",
      "Executing step [5/5]\n",
      "\n",
      "[GIN] 2025/03/04 - 14:36:59 | 200 |  256.441116ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "Tool call:\n",
      "[ToolCall(function=Function(name='do_math', arguments={'a': 20, 'b': 0, 'op': ''}))]\n",
      "\n",
      "[GIN] 2025/03/04 - 14:37:00 | 200 |  503.105373ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "====================== Final Prompt Template ======================\n",
      "\n",
      "Goal: Add the temperature in Austin, TX to the temperature in Ann Arbor, MI and then write a poem whose rhyming scheme is based off the pronunciation of the combined temperatures.\n",
      "\n",
      "Step by step guide:\n",
      "1. Get current temperature in Austin, TX\n",
      "2. Get current temperature in Ann Arbor, MI\n",
      "3. Add the temperatures of Austin and Ann Arbor\n",
      "4. Get current time in degrees Celsius\n",
      "5. Generate a poem based on the combined temperature's pronunciation\n",
      "\n",
      "\n",
      "Results from each step:\n",
      "Step 1: Get current temperature in Austin, TX\n",
      "    The current temperature in Austin, TX is: 18Â°C\n",
      "Step 2: Get current temperature in Ann Arbor, MI\n",
      "    The current temperature in Ann Arbor, MI is: 2Â°C\n",
      "Step 3: Add the temperatures of Austin and Ann Arbor\n",
      "    20\n",
      "Step 4: Get current time in degrees Celsius\n",
      "    The current time is 14:36:59\n",
      "Step 5: Generate a poem based on the combined temperature's pronunciation\n",
      "    Nan\n",
      "\n",
      "Complete the goal. Respond only with the answer.\n",
      "\n",
      "===================================================================\n",
      "\n",
      "\n",
      "Final Response:\n",
      "Nan, a moment's pause, \n",
      "Frozen time, in twenty-degree cause,\n",
      "A stillness deep, as ice and snow,\n",
      "Fifteen forty-six, the clock does slow.\n"
     ]
    }
   ],
   "source": [
    "# user prompt\n",
    "user_prompt = \"Add the temperature in Austin, TX to the temperature in Ann Arbor, MI and then write a poem whose rhyming scheme is based off the pronunciation of the combined temperatures\"\n",
    "print(f\"User Prompt: {user_prompt}\\n\")\n",
    "\n",
    "# create a list of the available tools\n",
    "tools = [\n",
    "    generate_function_description(get_current_weather),\n",
    "    generate_function_description(get_current_time),\n",
    "    generate_function_description(do_math),\n",
    "    generate_function_description(get_duckduckgo_result),\n",
    "]\n",
    "functions = [f[\"function\"][\"name\"] for f in tools]\n",
    "nl = \"\\n\"\n",
    "print(f\"Tools the LLM has access to:\\n{nl.join([f'{f}' for f in functions])}\\n\")\n",
    "\n",
    "# Format our goal and tool descriptions into a step-by-step (sbs) guide template\n",
    "sbs_guide_template = f\"\"\"\\nGoal: {user_prompt}.\n",
    "\n",
    "Tools you can use:\n",
    "{tools}\n",
    "\n",
    "Break up your goal into subgoals that utilize these tools.\"\"\"\n",
    "\n",
    "print('Generating step by step guide...')\n",
    "\n",
    "# create the step by step guide template\n",
    "sbs_guide = llm_create_list(sbs_guide_template)\n",
    "\n",
    "# string to hold numbered list with step by step guide names\n",
    "sbs_guide_names = \"\"\n",
    "\n",
    "# Print guide to console or quit if we failed to make one\n",
    "if bool(sbs_guide):\n",
    "    # pull out list names\n",
    "    step_names = [item['name'] for item in sbs_guide['content']]\n",
    "\n",
    "    # print list to console for viewing\n",
    "    sbs_guide_names = f\"{nl.join([f'{i+1}. {name}' for i, name in enumerate(step_names)])}\\n\"\n",
    "    print(\"Generated Step by Step Guide:\\n\" + sbs_guide_names)\n",
    "else:\n",
    "    print(f\"No guide generated :(\")\n",
    "    sys.exit()\n",
    "\n",
    "# string array to hold responses from each step\n",
    "subgoal_results = []\n",
    "\n",
    "# Run tool calls on each step\n",
    "for i, step in enumerate(sbs_guide['content']):\n",
    "\n",
    "    print(f\"Executing step [{i+1}/{len(sbs_guide['content'])}]\\n\")\n",
    "    \n",
    "    # formate subgoal prompt template\n",
    "    subgoal_prompt_template = f\"\"\"Goal: {user_prompt}.\n",
    "\n",
    "Step by step guide:\n",
    "{sbs_guide_names}\n",
    "\"\"\"\n",
    "\n",
    "    # print(subgoal_prompt_template)\n",
    "    \n",
    "    if i>0:\n",
    "        # build formated string of previous sub goal results      \n",
    "        past_results = \"\\n\".join([f\"\"\"Step {step_id+1}: {sbs_guide['content'][step_id]['name']}\n",
    "    {subgoal_results[step_id]}\"\"\" for step_id in range(0,i)])\n",
    "\n",
    "        # add to our prompt template\n",
    "        subgoal_prompt_template+= \"Results of previous steps:\\n\\n\" + past_results + \"\\n\\n\"\n",
    "\n",
    "    # add the final instruction to our prompt template\n",
    "    subgoal_prompt_template+=f\"\"\"Complete the current step: {step['name']}\n",
    "{step['description']}\n",
    "\"\"\"   \n",
    "\n",
    "    # print(f\"Subgoal Template {i+1}:\\n{subgoal_prompt_template}\")\n",
    "    \n",
    "    # get new subgoal results, attempt 10 times if we fail\n",
    "    for attempt in range(10):\n",
    "        try:\n",
    "            subgoal_results.append(llm_prompt_tool(subgoal_prompt_template,tools))\n",
    "            break\n",
    "        except:\n",
    "            if attempt == 9:\n",
    "                subgoal_results.append(\"Failed to execute tool.\")\n",
    "\n",
    "    # print(f\"\\nResponse to step {i+1}:\\n{subgoal_results[i]}\\n\\n\")\n",
    "\n",
    "# write out the results of each subgoal\n",
    "final_subgoal_results = \"\\n\".join([f\"\"\"Step {step_id+1}: {sbs_guide['content'][step_id]['name']}\n",
    "    {subgoal_results[step_id]}\"\"\" for step_id in range(len(subgoal_results))])\n",
    "\n",
    "# formate final prompt template\n",
    "final_prompt_template = f\"\"\"Goal: {user_prompt}.\n",
    "\n",
    "Step by step guide:\n",
    "{sbs_guide_names}\n",
    "\n",
    "Results from each step:\n",
    "{final_subgoal_results}\n",
    "\n",
    "Complete the goal. Respond only with the answer.\n",
    "\"\"\"\n",
    "\n",
    "# perform final prompt call\n",
    "response = llm_prompt(final_prompt_template)\n",
    "print(\"====================== Final Prompt Template ======================\\n\")\n",
    "print(final_prompt_template)\n",
    "print(\"===================================================================\\n\\n\")\n",
    "\n",
    "print(f\"Final Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Tutorial-March-2025",
   "language": "python",
   "name": "jupyter_kernel_config"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
